4.1数据分片
    4.1.1背景
        垂直分片:按照业务将不同的数据表分布在不同的数据库以减轻压力，例如user一个库，order一个库
        水平分片:按照固定规则分布在不同数据库。例如奇数在一个库，偶数在一个库
    4.1.2挑战
        基于XA的事务在大型互联网公司并不能得到满足，更多的是基于柔性事务保证最终一致性
    4.1.3目标
        尽量透明化数据库分库带来的影响，像使用单库一样操作简单
    4.1.4核心概念
        逻辑表：t_order_0~t_order_9统称为t_order
        真实表：torder0
        数据节点:数据分片的最小单元。由数据源名称和数据表组成，例：ds_0.t_order_0。
        绑定表：例如：t_order 表和 t_order_item 表，均按照 order_id 分片
        广播表：指所有的分片数据源中都存在的表，表结构和表中的数据在每个数据库中均完全一致。适用于数据量不大且需要与海量数据的表进行关联查询的场景，例如：字典表。
        单表：指所有的分片数据源中只存在唯一一张的表。适用于数据量不大且不需要做任何分片操作的场景。
        分片算法：目前提供 3 种分片算法。由于分片算法和业务实现紧密相关，因此并未提供内置分片算法，而是通过分片策略将各种场景提炼出来，提供更高层级的抽象，并提供接口让应用开发者自行实现分片算法。
            标准分片算法(StandardShardingAlgorithm):用于处理使用单一键作为分片键的 =、IN、BETWEEN AND、>、<、>=、<= 进行分片的场景。需要配合 StandardShardingStrategy 使用。
            复合分片算法(ComplexKeysShardingAlgorithm):用于处理使用多键作为分片键进行分片的场景,需要配合 ComplexShardingStrategy 使用
            Hint 分片算法(HintShardingAlgorithm):用于处理使用 Hint 行分片的场景。需要配合 HintShardingStrategy 使用。
        分片策略:
            标准分片策略:对sql中where的id进行提取分片
            复合分片策略:直接将分片键值组合以及分片操作符透传至分片算法，完全由应用开发者实现，提供最大的灵活度
            Hint 分片策略：通过 Hint 指定分片值而非从 SQL 中提取分片值的方式进行分片的策略，往往由登录id决定。
            不分片策略。
        分片策略配置(两种方式完全相同)：
            数据源分片策略：对应于 DatabaseShardingStrategy。用于配置数据被分配的目标数据源。
            表分片策略：对应于 TableShardingStrategy。
        自增主键生成策略
            通过在客户端生成自增主键替换以数据库原生自增主键的方式，做到分布式主键无重复。
        行表达式
            实现动机：统一规范，有效动态增加
            语法说明：Groovy 的语法
                例如：
                    ${begin..end} 表示范围区间
                    ${[unit1, unit2, unit_x]} 表示枚举值
                ${['online', 'offline']}_table${1..3}========》online_table1, online_table2, online_table3, offline_table1, offline_table2,offline_table3
            配置数据节点：db${0..1}.t_order${0..1}或者db$->{0..1}.t_order$->{0..1}
            配置分片算法:分为 10 个库，尾数为 0 的路由到后缀为 0 的数据源，尾数为 1 的路由到后缀为 1 的数据源，以此类推。ds${id % 10}
        分布式主键
            内置的主键生成器
                UUID
                SNOWFLAKE(默认算法)
        强制分片路由
            Hint
    4.1.5内核剖析
        ShardingSphere 的 3 个产品的数据分片主要流程是完全一致的。核心由 SQL 解析 => 执行器优化 =>SQL 路由 => SQL 改写 => SQL 执行 => 结果归并的流程组成。
        SQL 解析
            分为词法解析和语法解析。先通过词法解析器将 SQL 拆分为一个个不可再分的单词。再使用语法解析器对 SQL 进行理解，并最终提炼出解析上下文。解析上下文包括表、选择项、排序项、分组项、聚合函数、分页信息、查询条件以及可能需要修改的占位符的标记。
        执行器优化
            合并和优化分片条件，如 OR 等。
        SQL 路由
            根据解析上下文匹配用户配置的分片策略，并生成路由路径。目前支持分片路由和广播路由。
        SQL 改写
            将 SQL 改写为在真实数据库中可以正确执行的语句。SQL 改写分为正确性改写和优化改写。
        SQL 执行
            通过多线程执行器异步执行。
        结果归并
            将多个执行结果集归并以便于通过统一的 JDBC 接口输出。结果归并包括流式归并、内存归并和使用装饰者模式的追加归并这几种方式
        【图 5: SQL 抽象语法树】
        SQL 解析引擎
            1.4以前使用 Druid 作为 SQL 解析器。之后使用自研，仅提炼需要的部分，目前使用visit
            使用方式ParseTree tree = new SQLParserEngine(databaseType).parse(sql, useCache);
        分片路由
            直接路由
                使用 HintAPI 直接指定路由至库表，并且只分库不分表才会使用
            标准路由
                当分片运算符是等于号时，路由结果将落入单库（表），当分片运算符是 BETWEEN或 IN 时，则路由结果不一定落入唯一的库（表），因此一条逻辑 SQL 最终可能被拆分为多条用于执行的真实 SQL。
                例如:
                    SELECT * FROM t_order o JOIN t_order_item i ON o.order_id=i.order_id WHERE order_id IN (1, 2);
                    则可能被拆分成
                    SELECT * FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id=i.order_id WHERE order_id IN (1, 2);
                    SELECT * FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id=i.order_id WHERE order_id IN (1, 2);
            笛卡尔积路由
                在分片、绑定关系不明确时会使用
        广播路由
            全库表路由
                遍历所有数据库中的所有表，即in 语句等
            全库路由
                全库路由用于处理对数据库的操作，包括用于库设置的 SET 类型的数据库管理命令
            全实例路由
                全实例路由用于 DCL 操作，授权语句针对的是数据库的实例。无论一个实例中包含多少个 Schema，每个数据库的实例只执行一次
            单播路由
                它仅需要从任意库中的任意真实表中获取数据即可。
            阻断路由
                USE order_db等语句;在jdbc中无需切换schema。
        【图 6: 路由引擎结构】
        改写引擎
            标识符改写:将t_order表名改写成t_order_0
            补列:单表查询的order某个字段是不需要查出来的，但是分库则需要合并结果集
            批量拆分:分开在多个库执行
        【图 9: 改写引擎结构】
        连接模式(基于 maxConnectionSizePerQuery进行计算)
            内存限制模式:多线程
            连接限制模式:串行查询
        归并引擎
            遍历归并：只需将多个数据结果集合并为一个单向链表即可
            排序归并:由于在 SQL 中存在 ORDER BY 语句，因此每个数据结果集自身是有序的，因此只需要将数据结果集当前游标指向的数据值进行排序即可。这相当于对多个有序的数组进行排序，归并排序是最适合此场景的排序算法。
            分组归并
                流式分组归并： SQL 的排序项与分组项的字段以及排序类型（ASC 或 DESC）必须保持一致
                    SELECT name, SUM(score) FROM t_score GROUP BY name ORDER BY name;
                和内存分组归并：反之
                    SELECT name, SUM(score) FROM t_score GROUP BY name ORDER BY score DESC;
        不支持项
            不支持union
            不支持 case when中带逻辑表名
            不支持跨schema
            运算表达式和函数中的分片键会导致全路由
        分页支持
            Oracle 和 SQLServer的子查询分页支持，不支持 rownum + BETWEEN
            SQLServer的OFFSET FETCH
            MySQL 和 PostgreSQL 都支持 LIMIT
4.2分布式事务【简单点可以直接使用mysql官方的cluster】
    4.2.1背景
        支持ACID
        ||本地事务|两(三)阶段提交(XA)|柔性事务|
        |--|--|--|--|
        |业务改造|无||无|实现相关接口|
        |一致性|不支持|支持|最终一致|
        |隔离性|不支持|支持|业务方保证|
        |并发性能|无影响|严重衰退|略微衰退|
        |适合场景|业务方处理不一致|短事务&低并发|长事务&高并发|
    4.2.2挑战
        需要开发者自行选择合适的使用方式
    4.2.3目标
        整合现有成熟方案，为本地事务和柔性事务提供统一的分布式接口，并弥补当前方案的不足。提供一站式的分布式事务解决方案是 Apache ShardingSphere 分布式事务模块的主要设计目标。
    4.2.4核心概念
        XA两阶段事务：为数据库实现，缺点在于水平扩展时事务时间成倍增加
        Seata 柔性事务：阿里集团的成熟方案，其实现了4种方案，以AT模式侵入性最小(除了XA)。其 AT 事务的目标是在微服务架构下，提供增量事务 ACID 语意，让开发者像使用本地事务一样。事务模型包含 TM (事务管理器)，RM (资源管理器) 和 TC (事务协调器)。
            TC 是一个独立部署的服务，TM 和 RM 以 jar 包的方式同业务应用一同部署，它们同 TC 建立长连接，在整个事务生命周期内，保持远程通信。TM 是全局事务的发起方，负责全局事务的开启，提交和回滚。RM 是全局事务的参与者，负责分支事务的执行结果上报，并且通过 TC 的协调进行分支事务的提交和回滚。
            Seata 管理的分布式事务的典型生命周期：
                1. TM 要求 TC 开始一个全新的全局事务。TC 生成一个代表该全局事务的 XID。
                2. XID 贯穿于微服务的整个调用链。
                3. 作为该 XID 对应到的 TC 下的全局事务的一部分，RM 注册本地事务。
                4. TM 要求 TC 提交或回滚 XID 对应的全局事务。
                5. TC 驱动 XID 对应的全局事务下的所有分支事务完成提交或回滚。
    4.2.5实现原理
        XA 两阶段事务
            XAShardingSphereTransactionManager 为 ApacheShardingSphere 的分布式事务的 XA 实现类。它主要负责对多数据源进行管理和适配，并且将相应事务的开启、提交和回滚操作委托给具体的 XA 事务管理器。
            【图 20: XA 事务实现原理】
        Seata 柔性事务
            整合 Seata AT 事务时，需要将 TM，RM 和 TC 的模型融入 Apache ShardingSphere 的分布式事务生态中。在数据库资源上，Seata 通过对接 DataSource 接口，让 JDBC 操作可以同 TC 进行远程通信。同样，ApacheShardingSphere 也是面向 DataSource 接口，对用户配置的数据源进行聚合。因此，将 DataSource封装为基于 Seata 的 DataSource 后，就可以将 Seata AT 事务融入到 Apache ShardingSphere 的分片生态中。
            【图 21: 柔性事务 Seata】
            开启全局事务
                TM 控制全局事务的边界，TM 通过向 TC 发送 Begin 指令，获取全局事务 ID，所有分支事务通过此全局事务 ID，参与到全局事务中；全局事务 ID 的上下文存放在当前线程变量中。
            执行真实分片sql
                处于 Seata 全局事务中的分片 SQL 通过 RM 生成 undo 快照，并且发送 participate 指令至 TC，加入到全局事务中。由于 Apache ShardingSphere 的分片物理 SQL 采取多线程方式执行，因此整合 Seata AT事务时，需要在主线程和子线程间进行全局事务 ID 的上下文传递。
            提交或回滚事务
                提交 Seata 事务时，TM 会向 TC 发送全局事务的提交或回滚指令，TC 根据全局事务 ID 协调所有分支事务进行提交或回滚。
    4.2.6使用规范
        CAP环境有所取舍，需要由开发者自行确定
        本地事务
            支持项
                完全支持非跨库事务，例如：仅分表，或分库但是路由的结果在单库中；
                完全支持因逻辑异常导致的跨库事务。例如：同一事务中，跨两个库更新。更新完毕后，抛出空指针，则两个库的内容都能回滚。
            不支持项
                 不支持因网络、硬件异常导致的跨库事务。例如：同一事务中，跨两个库更新，更新完毕后、未提交之前，第一个库宕机，则只有第二个库数据提交。
        XA 两阶段事务
            支持项
                支持数据分片后的跨库事务；
                两阶段提交保证操作的原子性和数据的强一致性；
                服务宕机重启后，提交/回滚中的事务可自动恢复；
                支持同时使用 XA 和非 XA 的连接池。
            不支持项
                服务宕机后，在其它机器上恢复提交/回滚中的数据。
        Seata 柔性事务
            支持项
                支持数据分片后的跨库事务；
                支持 RC 隔离级别；
                通过 undo 快照进行事务回滚；
                支持服务宕机后的，自动恢复提交中的事务。
            不支持项
                不支持除 RC 之外的隔离级别。
            待优化项
                Apache ShardingSphere 和 Seata 重复 SQL 解析。
4.3读写分离【注：不建议做读写分离，特别是分布式环境】
    4.3.1背景
        加快读取速度
    4.3.2挑战
        多库数据不一致问题
    4.3.3目标
        透明化读写分离
    4.3.4核心概念
        主库
        从库
        主从同步
        负载均衡策略
    4.3.5使用规范
        支持项
            提供一主多从的读写分离配置，可独立使用，也可配合分库分表使用
            独立使用读写分离支持 SQL 透传
            基于 Hint 的强制主库路由
        不支持项
            主库和从库的数据同步
            主库和从库的数据同步延迟导致的数据不一致
            主库双写或多写
            跨主库和从库之间的事务的数据不一致。主从模型中，事务中读写均用主库
4.4分布式治理
    4.4.1背景
        多节点
    4.4.2挑战
        分布式治理的挑战，主要在于集群管理的复杂性，以及如何以统一和标准的方式对接各种第三方集成组件。
    4.4.3目标
        支持 Zookeeper/etcd，管理数据源、规则和策略的配置，管理各个 Proxy 实例的状态。
        支持 OpenTracing/Skywalking 集成，实现调用链的跟踪；
    4.4.4治理
        注册中心
            配置集中化：越来越多的运行时实例，使得散落的配置难于管理，配置不同步导致的问题十分严重。将配置集中于配置中心，可以更加有效进行管理。
            配置动态化：配置修改后的分发，是配置中心可以提供的另一个重要能力。它可支持数据源和规则的动态切换。
            存放运行时的动态/临时状态数据，比如可用的 ShardingSphere 的实例，需要禁用或熔断的数据源等。
            提供熔断数据库访问程序对数据库的访问和禁用从库的访问的编排治理能力。治理模块仍然有大量未完成的功能（比如流控等）。
        第三方组件依赖
            Apache ShardingSphere 在数据库治理模块使用 SPI 方式载入数据到配置中心和注册中心，进行实例熔断和数据库禁用。目前，Apache ShardingSphere 内部支持 ZooKeeper，Etcd 等常用的配置中心/注册中心。此外，开发者可以使用其他第三方组件，并通过 SPI 的方式注入到 Apache ShardingSphere，从而使用该配置中心和注册中心，实现数据库治理功能。
4.5 弹性伸缩
    4.5.1背景
        如何安全简单地将数据迁移至水平分片的数据库上，一直以来都是一个迫切的需求
    4.5.2 简介
        图 31: 结构总揽
    4.5.3 挑战
        如何找到一种方式，即能支持各类不同用户的分片策略和算法，又能高效地将数据节点进行伸缩，是弹性伸缩面临的第一个挑战；
    4.5.4 目标
        支持各类用户自定义的分片策略，减少用户在数据伸缩及迁移时的重复工作及业务影响，提供一站式的通用弹性伸缩解决方案，是 Apache ShardingSphere 弹性伸缩的主要设计目标。
    4.5.5 状态
        当前处于 alpha 开发阶段。
        【图 32: 路线图】
    4.5.7 实现原理
        目前的弹性伸缩解决方案为：临时地使用两个数据库集群，伸缩完成后切换的方式实现。
4.6数据加密
4.7影子库压测
4.8Dist SQL
4.8可插拔架构
4.9测试引擎